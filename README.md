# crawling-hadoop-eco

# Panduan Penggunaan Hadoop dan Apache Airflow untuk Crawling Program

Proyek ini bertujuan untuk menunjukkan cara menggunakan Hadoop Ecosystem (HDFS) dan Apache Airflow untuk menjalankan dan menjadwalkan program crawling data. Kami akan menjelaskan langkah-langkah instalasi Hadoop dan Apache Airflow serta bagaimana menjalankan crawling program di lingkungan ini.

## Instalasi Hadoop

Untuk menginstal Hadoop, ikuti langkah-langkah berikut:

1. Ikuti panduan instalasi Hadoop di Windows di [tautan berikut](https://medium.com/republic-of-coders-india/guide-to-install-and-run-hadoop-on-windows-a0b64fe447b6).

2. Pastikan Hadoop berjalan dengan benar sebelum melanjutkan ke langkah berikutnya.

## Instalasi Apache Airflow

Untuk menginstal Apache Airflow, ikuti langkah-langkah berikut:

1. Ikuti panduan instalasi Apache Airflow di Docker di Windows di [tautan berikut](https://medium.com/@garc1a0scar/how-to-start-with-apache-airflow-in-docker-windows-902674ad1bbe).

2. Pastikan Docker berjalan dengan benar dan Apache Airflow dapat diakses di browser Anda.

## Persiapan Proyek Crawling

1. Clone repository proyek ini.

```shell
git clone <URL repository proyek>
